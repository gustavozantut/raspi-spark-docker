# Start from a base image with Java and Spark dependencies
FROM openjdk:17-jdk-bullseye

# Set environment variables
ENV SPARK_VERSION=3.5.3 \
    HADOOP_VERSION=3 \
    SPARK_HOME=/spark \
    JDBC_DRIVER_DIR=/usr/share/java \
    SPARK_MASTER_URL=spark://192.168.0.101:7077

# Install dependencies, download Spark and JDBC driver in a single layer
RUN apt-get update && \
    apt-get install -y wget python3-pip && \
    wget https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    tar -xvf spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    rm spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz && \
    mv spark-${SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} ${SPARK_HOME} && \
    mkdir -p ${JDBC_DRIVER_DIR} && \
    curl -o ${JDBC_DRIVER_DIR}/mssql-jdbc.jar https://repo1.maven.org/maven2/com/microsoft/sqlserver/mssql-jdbc/11.2.3.jre17/mssql-jdbc-11.2.3.jre17.jar

# Copy and install Python dependencies
COPY requirements.txt .
RUN pip install -r requirements.txt && rm requirements.txt

# Set Spark as the working directory
WORKDIR ${SPARK_HOME}

ENV SPARK_PUBLIC_DNS=192.168.0.210 \
    SPARK_LOCAL_IP=0.0.0.0

# Expose necessary Spark port
EXPOSE 8081

# Start the Spark worker
CMD ./sbin/start-worker.sh ${SPARK_MASTER_URL} && tail -f /dev/null
